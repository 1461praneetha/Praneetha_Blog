{
  
    
        "post0": {
            "title": "Title",
            "content": "%load_ext autoreload %autoreload 2 . %matplotlib inline import numpy as np import matplotlib.pyplot as plt import pandas as pd . incase of using mnist for the first time use this command to install mnist from the notebook - !pip install mnist Incase to get it from command line use pip install mnist . import mnist . train_images = mnist.train_images() train_labels = mnist.train_labels() . train_images.shape, train_labels.shape . ((60000, 28, 28), (60000,)) . test_images = mnist.test_images() test_labels = mnist.test_labels() . test_images.shape, test_labels.shape . ((10000, 28, 28), (10000,)) . image_index = 7777 # You may select anything up to 60,000 print(train_labels[image_index]) plt.imshow(train_images[image_index], cmap=&#39;Greys&#39;) . 8 . &lt;matplotlib.image.AxesImage at 0x1bcc7e1d888&gt; . Filter data to get 3 and 8 out . train_filter = np.where((train_labels == 3 ) | (train_labels == 8)) test_filter = np.where((test_labels == 3) | (test_labels == 8)) X_train, y_train = train_images[train_filter], train_labels[train_filter] X_test, y_test = test_images[test_filter], test_labels[test_filter] . X_train = X_train/255. #normalising the pixel values in the 0 to 1 range X_test = X_test/255. . y_train = 1*(y_train==3) y_test = 1*(y_test==3) . X_train.shape, X_test.shape . ((11982, 28, 28), (1984, 28, 28)) . X_train = X_train.reshape(X_train.shape[0], -1) X_test = X_test.reshape(X_test.shape[0], -1) X_train.shape, X_test.shape . ((11982, 784), (1984, 784)) . Building a classifier from kudzu . from kudzu.model import Model from kudzu.layer import Affine, Sigmoid, Relu from kudzu.loss import MSE from kudzu.optim import GD from kudzu.data import Data, Dataloader, Sampler from kudzu.train import Learner . class Config: pass config = Config() config.lr = 0.001 config.num_epochs = 200 config.bs = 50 . data = Data(X_train,y_train.reshape(-1,1)) loss = MSE() opt = GD(config.lr) sampler = Sampler(data, config.bs, shuffle = True) dl = Dataloader(data,sampler) . train_x = X_train test_x = X_test train_y = y_train.reshape(-1,1) test_y = y_test.reshape(-1,1) . layers = [Affine(&quot;first&quot;,784,100),Relu(&quot;first&quot;),Affine(&quot;second&quot;,100,100),Relu(&quot;second&quot;),Affine(&quot;third&quot;,100,2),Affine(&quot;final&quot;,2,1),Sigmoid(&quot;final&quot;)] model_nn = Model(layers) layers . xavier xavier xavier xavier . [&lt;kudzu.layer.Affine at 0x1bcd18cf148&gt;, &lt;kudzu.layer.Relu at 0x1bcd18e7b48&gt;, &lt;kudzu.layer.Affine at 0x1bcd18cf508&gt;, &lt;kudzu.layer.Relu at 0x1bcd18cfb08&gt;, &lt;kudzu.layer.Affine at 0x1bcd18cf7c8&gt;, &lt;kudzu.layer.Affine at 0x1bcd18cf288&gt;, &lt;kudzu.layer.Sigmoid at 0x1bcd18cf5c8&gt;] . Created a subclass &quot;ClfCallback&quot; of &quot;Callback&quot; class in callbacks.py and import in it . from kudzu.callbacks import ClfCallback . learner_nn = Learner(loss, model_nn, opt, config.num_epochs) acc_nn = ClfCallback(learner_nn,config.bs,train_x,test_x,train_y,test_y) learner_nn.set_callbacks([acc_nn]) . learner_nn.train_loop(dl) . Epoch 0 Loss 0.020414649894784784 train accuracy 0.9770489066933734, test accuracy 0.9768145161290323 Epoch 10 Loss 0.019968426254200165 train accuracy 0.9775496578200634, test accuracy 0.9768145161290323 Epoch 20 Loss 0.019541294650811187 train accuracy 0.9782173259889835, test accuracy 0.9778225806451613 Epoch 30 Loss 0.01913308413878309 train accuracy 0.9788015356367885, test accuracy 0.9773185483870968 Epoch 40 Loss 0.01873634819604516 train accuracy 0.9793022867634785, test accuracy 0.9773185483870968 Epoch 50 Loss 0.01836061225947461 train accuracy 0.9797195793690536, test accuracy 0.9773185483870968 Epoch 60 Loss 0.01798892083297192 train accuracy 0.9798864964112836, test accuracy 0.9773185483870968 Epoch 70 Loss 0.017634590969136516 train accuracy 0.9805541645802036, test accuracy 0.9773185483870968 Epoch 80 Loss 0.017290040402642908 train accuracy 0.9811383742280086, test accuracy 0.9778225806451613 Epoch 90 Loss 0.016957089304162738 train accuracy 0.9814722083124687, test accuracy 0.9783266129032258 Epoch 100 Loss 0.016634693097287597 train accuracy 0.9815556668335838, test accuracy 0.9788306451612904 Epoch 110 Loss 0.01632350181923612 train accuracy 0.9820564179602738, test accuracy 0.9793346774193549 Epoch 120 Loss 0.016013025658426147 train accuracy 0.9828910031714237, test accuracy 0.9793346774193549 Epoch 130 Loss 0.015713789950336825 train accuracy 0.9832248372558838, test accuracy 0.9793346774193549 Epoch 140 Loss 0.015415343746153649 train accuracy 0.9835586713403438, test accuracy 0.9793346774193549 Epoch 150 Loss 0.015131389534679164 train accuracy 0.9839759639459189, test accuracy 0.9798387096774194 Epoch 160 Loss 0.014847846744120645 train accuracy 0.9840594224670339, test accuracy 0.9798387096774194 Epoch 170 Loss 0.014574919388276836 train accuracy 0.9842263395092639, test accuracy 0.9798387096774194 Epoch 180 Loss 0.014309288778595204 train accuracy 0.9844767150726089, test accuracy 0.9803427419354839 Epoch 190 Loss 0.014044022389197067 train accuracy 0.9849774661992989, test accuracy 0.9808467741935484 . 0.011378928193043269 . plt.plot(acc_nn.accuracies,label = &quot;Train Accuracies&quot;) plt.plot(acc_nn.test_accuracies,&quot;r-&quot;,label = &quot;Test Accuracies&quot;) plt.ylim(0.6,1) plt.title(&quot;Classification by NN&quot;) plt.legend(loc = &quot;lower right&quot;) . &lt;matplotlib.legend.Legend at 0x1bcf9904b88&gt; . #hints taken from TA sessions #upto 3 three layers for visualisation model_vis = Model(layers[:-2]) . vis = model_vis(test_x) plt.figure(figsize =(14,7)) plt.subplot(1,2,1) plt.scatter(vis[:,0],vis[:,1],alpha = 0.09,c = y_test.ravel()); plt.colorbar() plt.subplot(1,2,2) plt.scatter(vis[:,0],vis[:,1],c = y_test.ravel()); plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x1bce694ae08&gt; . model_prob = Model(layers[-2:]) . xgrid = np.linspace(-4, 1, 100) ygrid = np.linspace(-7.5, 7.5, 100) xg, yg = np.meshgrid(xgrid, ygrid) # xg and yg are now both 100X100 -&gt; we need to conver them to single arrays . xg_interim = np.ravel(xg) yg_interim = np.ravel(yg) # xg_interim, yg_interim are now arrays of len 10000, now we will stack them and then transpose to get desired shape of n rows, 2 columns . X_interim = np.vstack((xg_interim, yg_interim)) X = X_interim.T . # We want a shape of n rows and 2 columns in order to be able to feed this to last affine # This last affine takes only two columns, hence the above transformation probability_contour = model_prob(X).reshape(100,100) . plt.figure(figsize=(14,14)) plt.subplot(2,1,1) plt.scatter(vis[:,0], vis[:,1],alpha = 0.1,c = y_test.ravel()) contours = plt.contour(xg,yg,probability_contour) plt.clabel(contours, inline = True ); plt.title(&quot;Probability Contour that distinguishes one data class from the other showing misclassified points part 1&quot;); plt.subplot(2,1,2) plt.scatter(vis[:,0], vis[:,1], c = y_test.ravel()) contours = plt.contour(xg,yg,probability_contour) plt.clabel(contours, inline = True ); plt.title(&quot;Probability Contour that distinguishes one data class from the other showing misclassified points part 2&quot;); . layers_logreg = [Affine(&quot;first&quot;,784,1),Sigmoid(&quot;first&quot;)] model_logreg = Model(layers_logreg) . xavier . learner_logreg = Learner(loss, model_logreg, opt, config.num_epochs) acc_logreg = ClfCallback(learner_logreg,config.bs,train_x,test_x,train_y,test_y) learner_logreg.set_callbacks([acc_logreg]) . learner_logreg.train_loop(dl) . Epoch 0 Loss 0.2792090242997579 train accuracy 0.5315473209814722, test accuracy 0.5761088709677419 Epoch 10 Loss 0.10640380421536118 train accuracy 0.9080287097312636, test accuracy 0.9223790322580645 Epoch 20 Loss 0.08033746928305888 train accuracy 0.92797529627775, test accuracy 0.9455645161290323 Epoch 30 Loss 0.06873467432650342 train accuracy 0.9369053580370555, test accuracy 0.9511088709677419 Epoch 40 Loss 0.06188803429013372 train accuracy 0.942330161909531, test accuracy 0.954133064516129 Epoch 50 Loss 0.05727703267475671 train accuracy 0.9454181271907862, test accuracy 0.9561491935483871 Epoch 60 Loss 0.05391986894023762 train accuracy 0.9472542146553163, test accuracy 0.9566532258064516 Epoch 70 Loss 0.05133960523477456 train accuracy 0.9492572191620764, test accuracy 0.9591733870967742 Epoch 80 Loss 0.04927959221439201 train accuracy 0.9512602236688366, test accuracy 0.9601814516129032 Epoch 90 Loss 0.04758730570145927 train accuracy 0.9527624770489067, test accuracy 0.9616935483870968 Epoch 100 Loss 0.04616418285032355 train accuracy 0.9542647304289767, test accuracy 0.9642137096774194 Epoch 110 Loss 0.044946158896190525 train accuracy 0.9545985645134368, test accuracy 0.9642137096774194 Epoch 120 Loss 0.04388799105710623 train accuracy 0.9557669838090469, test accuracy 0.9642137096774194 Epoch 130 Loss 0.04295741654384272 train accuracy 0.956601569020197, test accuracy 0.9642137096774194 Epoch 140 Loss 0.042129633053059036 train accuracy 0.957102320146887, test accuracy 0.9647177419354839 Epoch 150 Loss 0.04138795453271482 train accuracy 0.9578534468369221, test accuracy 0.9642137096774194 Epoch 160 Loss 0.04071680955932315 train accuracy 0.958604573526957, test accuracy 0.9642137096774194 Epoch 170 Loss 0.04010724495040125 train accuracy 0.9586880320480721, test accuracy 0.9642137096774194 Epoch 180 Loss 0.03954764663837227 train accuracy 0.9591053246536472, test accuracy 0.9637096774193549 Epoch 190 Loss 0.039034123794101254 train accuracy 0.9596060757803372, test accuracy 0.9642137096774194 . 0.04355349767802931 . plt.plot(acc_logreg.accuracies,label = &quot;Train Accuracies&quot;) plt.plot(acc_logreg.test_accuracies,&quot;r-&quot;,label = &quot;Test Accuracies&quot;) plt.ylim(0.6,1) plt.title(&quot;Classification by Logistic Regression&quot;) plt.legend(loc = &quot;lower right&quot;) . &lt;matplotlib.legend.Legend at 0x1bcf98655c8&gt; . plt.figure(figsize = (12,5)) plt.subplot(1,2,1) plt.plot(acc_nn.accuracies,label = &quot;Train Accuracies&quot;) plt.plot(acc_nn.test_accuracies,&quot;r-&quot;,label = &quot;Test Accuracies&quot;) plt.ylim(0.6,1) plt.title(&quot;Classification by NN&quot;) plt.legend(loc = &quot;lower right&quot;) plt.subplot(1,2,2) plt.plot(acc_logreg.accuracies,label = &quot;Train Accuracies&quot;) plt.plot(acc_logreg.test_accuracies,&quot;r-&quot;,label = &quot;Test Accuracies&quot;) plt.ylim(0.6,1) plt.title(&quot;Classification by Logistic Regression&quot;) plt.legend(loc = &quot;lower right&quot;) . &lt;matplotlib.legend.Legend at 0x1bcd1f49508&gt; .",
            "url": "https://1461praneetha.github.io/Praneetha_Blog/2020/08/10/distinguish-digits.html",
            "relUrl": "/2020/08/10/distinguish-digits.html",
            "date": " • Aug 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "COVID-19 Overview - India",
            "content": "India . Last update: 28-Jul-20 . Confirmed cases: . 1514800 (+1481448) . Confirmed deaths: . 34121 (+770) . #collapse df = dft_ct_new_cases.copy() # print(df) df.loc[&#39;Total&#39;] = df.sum() n = 5 ax = [] fig = plt.figure(figsize = (16,20)) gs = fig.add_gridspec(n+3, 3) # gs = fig.add_gridspec(2, 3) ax0 = fig.add_subplot(gs[0, :]) ef = df.loc[&#39;Total&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax0.bar(ef.date,ef.Total,alpha=0.3,color=&#39;#007acc&#39;) ax0.plot(ef.date,ef.Total , marker=&quot;o&quot;, color=&#39;#007acc&#39;) ax0.xaxis.set_major_locator(mdates.WeekdayLocator()) ax0.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax0.text(0.02, 0.5,&#39;India daily case count&#39;, transform = ax0.transAxes, fontsize=25); ax0.spines[&#39;right&#39;].set_visible(False) ax0.spines[&#39;top&#39;].set_visible(False) ax1 = fig.add_subplot(gs[1, :]) ef = df.loc[&#39;Delhi&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax1.bar(ef.date,ef.Delhi,alpha=0.3,color=&#39;#007acc&#39;) ax1.plot(ef.date,ef.Delhi , marker=&quot;o&quot;, color=&#39;#007acc&#39;) ax1.xaxis.set_major_locator(mdates.WeekdayLocator()) ax1.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax1.text(0.02, 0.5,&#39;Capital-Delhi&#39;, transform = ax1.transAxes, fontsize=25); ax1.spines[&#39;right&#39;].set_visible(False) ax1.spines[&#39;top&#39;].set_visible(False) ax2 = fig.add_subplot(gs[2,0]) ef = df.loc[&#39;Andhra Pradesh&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax2.bar(ef.date, ef[&#39;Andhra Pradesh&#39;],color = &#39;#007acc&#39;,alpha=0.5) ax2.xaxis.set_major_locator(mdates.WeekdayLocator()) ax2.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax2.set_xticks(ax2.get_xticks()[::3]) maxyval = ef[&#39;Andhra Pradesh&#39;].max() ax2.set_ylim([0,maxyval]) ax2.text(0.05, 0.5,&#39;Andhra Pradesh&#39;, transform = ax2.transAxes, fontsize=20); ax2.spines[&#39;right&#39;].set_visible(False) ax2.spines[&#39;top&#39;].set_visible(False) ax3 = fig.add_subplot(gs[2,1]) ef = df.loc[&#39;Tamil Nadu&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax3.bar(ef.date, ef[&#39;Tamil Nadu&#39;],color = &#39;#007acc&#39;,alpha=0.5,) ax3.xaxis.set_major_locator(mdates.WeekdayLocator()) ax3.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax3.set_xticks(ax3.get_xticks()[::3]) ax3.text(0.05, 0.5,&#39;Tamil Nadu&#39;, transform = ax3.transAxes, fontsize=20); ax3.spines[&#39;right&#39;].set_visible(False) ax3.spines[&#39;top&#39;].set_visible(False) ax4 = fig.add_subplot(gs[2,2]) ef = df.loc[&#39;Maharashtra&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax4.bar(ef.date, ef.Maharashtra,color = &#39;#007acc&#39;,alpha=0.5) ax4.set_xticks([]) ax4.xaxis.set_major_locator(mdates.WeekdayLocator()) ax4.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax4.set_xticks(ax4.get_xticks()[::3]) ax4.spines[&#39;right&#39;].set_visible(False) ax4.spines[&#39;top&#39;].set_visible(False) ax4.text(0.05, 0.5,&#39;Maharashtra&#39;, transform = ax4.transAxes, fontsize=20) for i in range(n): ax.append(fig.add_subplot(gs[i+3,:])) ef = df.iloc[i+3].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax[i].bar(ef.date,ef.iloc[:,-1],color = &#39;#007acc&#39;,alpha=0.3) ax[i].plot(ef.date,ef.iloc[:,-1],marker=&#39;o&#39;,color=&#39;#007acc&#39;) ax[i].text(0.02,0.5,f&#39;{ef.columns.values[-1]}&#39;,transform = ax[i].transAxes, fontsize = 20); ax[i].xaxis.set_major_locator(mdates.WeekdayLocator()) ax[i].xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax[i].set_ylim([0,7000]) ax[i].spines[&#39;right&#39;].set_visible(False) ax[i].spines[&#39;top&#39;].set_visible(False) plt.tight_layout() . . states Cases PCases Deaths PDeaths Cases (+) Deaths (+) Fatality Rate Maharashtra 391440 13882 14164 13882 377558 282 3.62 Tamil Nadu 227688 3571 3659 3571 224117 88 1.61 Delhi 132275 3853 3881 3853 128422 28 2.93 Andhra Pradesh 110297 1090 1148 1090 109207 58 1.04 Karnataka 107001 1962 2064 1962 105039 102 1.93 Uttar Pradesh 73951 1456 1497 1456 72495 41 2.02 West Bengal 62964 1411 1449 1411 61553 38 2.30 Gujarat 57982 2348 2372 2348 55634 24 4.09 Telangana 57142 471 480 471 56671 9 0.84 Bihar 43591 255 269 255 43336 14 0.62 Rajasthan 38636 633 644 633 38003 11 1.67 Assam 34846 90 92 90 34756 2 0.26 Haryana 32876 397 406 397 32479 9 1.23 Madhya Pradesh 29217 821 831 821 28396 10 2.84 Orissa 28107 181 189 181 27926 8 0.67 Kerala 20895 64 68 64 20831 4 0.33 Jammu and Kashmir 18879 321 333 321 18558 12 1.76 Punjab 14378 318 336 318 14060 18 2.34 Jharkhand 9563 90 94 90 9473 4 0.98 Goa 5287 36 36 36 5251 0 0.68 Tripura 4287 17 21 17 4270 4 0.49 Pondicherry 3013 43 47 43 2970 4 1.56 Himachal Pradesh 2330 13 13 13 2317 0 0.56 Manipur 2317 0 0 0 2317 0 0.00 Nagaland 1460 5 4 5 1455 0 0.27 Arunachal Pradesh 1330 3 3 3 1327 0 0.23 Chandigarh 934 14 14 14 920 0 1.50 Meghalaya 779 5 5 5 774 0 0.64 Sikkim 592 1 1 1 591 0 0.17 Mizoram 384 0 0 0 384 0 0.00 Andaman and Nicobar Islands 359 1 1 1 358 0 0.28 Daman and Diu 0 0 0 0 0 0 NaN Lakshadweep 0 0 0 0 0 0 NaN .",
            "url": "https://1461praneetha.github.io/Praneetha_Blog/2020/08/10/Praneetha_Blog.html",
            "relUrl": "/2020/08/10/Praneetha_Blog.html",
            "date": " • Aug 10, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://1461praneetha.github.io/Praneetha_Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": "Posts .",
          "url": "https://1461praneetha.github.io/Praneetha_Blog/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://1461praneetha.github.io/Praneetha_Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}